{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/ghansham/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordnet is one of the corpus inside the nlp package \n",
    "# nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wordnet\u001b[39m.\u001b[39;49msynset(\u001b[39m\"\u001b[39;49m\u001b[39mdog\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1434\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynset\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m   1433\u001b[0m     \u001b[39m# split name into lemma, part of speech and synset number\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m     lemma, pos, synset_index_str \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mrsplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m   1435\u001b[0m     synset_index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(synset_index_str) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1437\u001b[0m     \u001b[39m# get the offset for this synset\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "wordnet.synset(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the similar elements to a word in wordnet \n",
    "z=wordnet.synsets(\"back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('back.n.01'),\n",
       " Synset('rear.n.05'),\n",
       " Synset('back.n.03'),\n",
       " Synset('back.n.04'),\n",
       " Synset('spinal_column.n.01'),\n",
       " Synset('binding.n.05'),\n",
       " Synset('back.n.07'),\n",
       " Synset('back.n.08'),\n",
       " Synset('back.n.09')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so let us see the word but we want see where word acts like perticular pos\n",
    "wordnet.synsets(\"back\",pos=wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('back.n.01')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so lets go further perticular for that we use \n",
    "# synset singular form of synsets \n",
    "# wordnet.synset('dog.n.02')\n",
    "wordnet.synset(\"back.n.01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Synset"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=wordnet.synset(\"dog.n.01\")\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see some of the function which we can apply on synset object \n",
    "# defination function will give us the defination of word \n",
    "a.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun.animal\n",
      "noun.person\n",
      "noun.food\n",
      "a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n"
     ]
    }
   ],
   "source": [
    "# lexname will give the reference about the pos and for whom word is used  \n",
    "a=wordnet.synset(\"dog.n.01\")\n",
    "print(a.lexname())\n",
    "a=wordnet.synset(\"dog.n.02\")\n",
    "print(a.lexname())\n",
    "a=wordnet.synset(\"dog.n.05\") #hot dog \n",
    "print(a.lexname())\n",
    "print(a.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for dog.n.01\n",
      "['dog', 'domestic_dog', 'Canis_familiaris'] \n",
      "\n",
      "for dog.n.02\n",
      "['frump', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# lemma_names() will print the words related to that meaning of word \n",
    "a=wordnet.synset(\"dog.n.01\")\n",
    "\n",
    "# function lemma will return lemma object for words \n",
    "# print(a.lemmas())\n",
    "\n",
    "# function lemma_names will return the list of the words which are taken from\n",
    "# that perticular meaning of the word \n",
    "print(\"for dog.n.01\")\n",
    "print(a.lemma_names(),\"\\n\")\n",
    "\n",
    "\n",
    "# for each meaning we will get different set of lemmas \n",
    "a=wordnet.synset(\"dog.n.02\")\n",
    "# print(a.lemmas())\n",
    "print(\"for dog.n.02\")\n",
    "print(a.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in :  Synset('canine.n.02')  are :  ['canine', 'canid']\n",
      "words in :  Synset('domestic_animal.n.01')  are :  ['domestic_animal', 'domesticated_animal']\n"
     ]
    }
   ],
   "source": [
    "a=wordnet.synset(\"dog.n.01\")\n",
    "# hypernym function will give hypernym for that sense of word \n",
    "# it will return a list \n",
    "b=a.hypernyms()\n",
    "for i in b:\n",
    "    print(\"words in : \",i,\" are : \",i.lemma_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in :  Synset('basenji.n.01')  are :  ['basenji']\n",
      "words in :  Synset('corgi.n.01')  are :  ['corgi', 'Welsh_corgi']\n",
      "words in :  Synset('cur.n.01')  are :  ['cur', 'mongrel', 'mutt']\n",
      "words in :  Synset('dalmatian.n.02')  are :  ['dalmatian', 'coach_dog', 'carriage_dog']\n",
      "words in :  Synset('great_pyrenees.n.01')  are :  ['Great_Pyrenees']\n",
      "words in :  Synset('griffon.n.02')  are :  ['griffon', 'Brussels_griffon', 'Belgian_griffon']\n",
      "words in :  Synset('hunting_dog.n.01')  are :  ['hunting_dog']\n",
      "words in :  Synset('lapdog.n.01')  are :  ['lapdog']\n",
      "words in :  Synset('leonberg.n.01')  are :  ['Leonberg']\n",
      "words in :  Synset('mexican_hairless.n.01')  are :  ['Mexican_hairless']\n",
      "words in :  Synset('newfoundland.n.01')  are :  ['Newfoundland', 'Newfoundland_dog']\n",
      "words in :  Synset('pooch.n.01')  are :  ['pooch', 'doggie', 'doggy', 'barker', 'bow-wow']\n",
      "words in :  Synset('poodle.n.01')  are :  ['poodle', 'poodle_dog']\n",
      "words in :  Synset('pug.n.01')  are :  ['pug', 'pug-dog']\n",
      "words in :  Synset('puppy.n.01')  are :  ['puppy']\n",
      "words in :  Synset('spitz.n.01')  are :  ['spitz']\n",
      "words in :  Synset('toy_dog.n.01')  are :  ['toy_dog', 'toy']\n",
      "words in :  Synset('working_dog.n.01')  are :  ['working_dog']\n"
     ]
    }
   ],
   "source": [
    "a=wordnet.synset(\"dog.n.01\")\n",
    "# function to print hyponyms of words \n",
    "# it will return a list \n",
    "b=a.hyponyms()\n",
    "for i in b:\n",
    "    print(\"words in : \",i,\" are : \",i.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(Synset('animal.n.01'), 2),\n",
       " (Synset('animal.n.01'), 7),\n",
       " (Synset('canine.n.02'), 1),\n",
       " (Synset('carnivore.n.01'), 2),\n",
       " (Synset('chordate.n.01'), 6),\n",
       " (Synset('dog.n.01'), 0),\n",
       " (Synset('domestic_animal.n.01'), 1),\n",
       " (Synset('entity.n.01'), 8),\n",
       " (Synset('entity.n.01'), 13),\n",
       " (Synset('living_thing.n.01'), 4),\n",
       " (Synset('living_thing.n.01'), 9),\n",
       " (Synset('mammal.n.01'), 4),\n",
       " (Synset('object.n.01'), 6),\n",
       " (Synset('object.n.01'), 11),\n",
       " (Synset('organism.n.01'), 3),\n",
       " (Synset('organism.n.01'), 8),\n",
       " (Synset('physical_entity.n.01'), 7),\n",
       " (Synset('physical_entity.n.01'), 12),\n",
       " (Synset('placental.n.01'), 3),\n",
       " (Synset('vertebrate.n.01'), 5),\n",
       " (Synset('whole.n.02'), 5),\n",
       " (Synset('whole.n.02'), 10)}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the hypernym function will give you distance between word to its hypernyms \n",
    "# this is sorted the number at end is distance of that attribute from the word \n",
    "# like dog-->dog =0 \n",
    "# if word is there in multiplt occurance you can notice it too \n",
    "a.hypernym_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('domestic_animal.n.01'),\n",
       " Synset('dog.n.01')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will show ypu path from entity to that word \n",
    "a.hypernym_paths()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eng', 'als', 'arb', 'bul', 'cmn', 'dan', 'ell', 'fin', 'fra', 'heb', 'hrv', 'isl', 'ita', 'ita_iwn', 'jpn', 'cat', 'eus', 'glg', 'spa', 'ind', 'zsm', 'nld', 'nno', 'nob', 'pol', 'por', 'ron', 'lit', 'slk', 'slv', 'swe', 'tha'])\n",
      "32\n",
      "[Synset('dog.n.01'), Synset('spy.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# also wordnet knows a large set of languages and you can use it as the translator also \n",
    "print(wordnet.langs())\n",
    "print(len(wordnet.langs()))\n",
    "# wordnet knows as of now 32 languages \n",
    "a=wordnet.synsets(b'\\xe7\\x8a\\xac'.decode('utf-8'),lang=\"jpn\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to gice lowest common hypernym \n",
    "# lcs is basic function with wordnet\n",
    "\n",
    "def lcs(a,b):\n",
    "    aa=wordnet.synsets(a)[0]\n",
    "    bb=wordnet.synsets(b)[0]\n",
    "    # print(b,c)\n",
    "    return aa.lowest_common_hypernyms(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('artifact.n.01')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs(\"bench\",\"wall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing IC from wordnet \n",
    "from nltk.corpus import brown\n",
    "# so we are now going to create a ic for genesis database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ghansham/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownIc=wordnet.ic(brown,False,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the resnikSimilarity value \n",
    "# formula :IC(c)=−log p(lcs(c,c2))\n",
    "\n",
    "def resnikSimilarity(a):\n",
    "\n",
    "    aa=wordnet.synsets(a)[0]\n",
    "    return aa.res_similarity(aa,brownIc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.857706440000147"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnikSimilarity(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are different ways are there to find the lin similarity \n",
    "# as we know that inorder to find the similarity between the words with the help\n",
    "# of lin similarity then we are going to use ic that is Information Content \n",
    "# i will use both here \n",
    "\n",
    "# formula for lin_similarity is \n",
    "    # lin_similarity = 2*IC(c1,c2)/(IC(c1)+IC(c2))\n",
    "    \n",
    "def lin_similarity(a,b):\n",
    "    # taking top most meaning as the synset and do further proceding on it \n",
    "    aa=wordnet.synsets(a)[0]\n",
    "    bb=wordnet.synsets(b)[0]\n",
    "\n",
    "    # the value which we get with resmik_similarity with itself is ic itself\n",
    "    # lets calculate and then return the value for the lin_similarity\n",
    "    linsim=2*(aa.res_similarity(bb,brownIc))/(resnikSimilarity(a)+resnikSimilarity(b))\n",
    "    \n",
    "    return linsim#,aa.lin_similarity(bb,brownIc)\n",
    "    # print(aa,bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19749670086034857"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is function which is calling the lin similarity\n",
    "lin_similarity(\"plant\",\"tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jiang-Conrath distance \n",
    "def jcDistance(a,b):\n",
    "    # jcDistance(c1,c2)=IC(c1)+IC(c2)-2*IC(lcs(c1,c2))\n",
    "    aa=wordnet.synsets(a)[0]\n",
    "    bb=wordnet.synsets(b)[0]\n",
    "    # d=resnikSimilarity(a)+resnikSimilarity(b)-2*(z.res_similarity(z,brownIc))\n",
    "    return 1/aa.jcn_similarity(bb,brownIc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.261263382520495"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jcDistance(\"cat\",\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leacock-Chodorow similarity\n",
    "def leacockChodorowSimilarity(a,b):\n",
    "    aa=wordnet.synsets(a)[0]\n",
    "    bb=wordnet.synsets(b)[0]\n",
    "    return aa.lch_similarity(bb,brownIc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649974374866046"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leacockChodorowSimilarity(\"car\",\"brush\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
