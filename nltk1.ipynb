{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udrHStR-gY76",
        "outputId": "b725655b-393c-4790-b49e-d04bed78a75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/ghansham/.local/lib/python3.8/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /home/ghansham/.local/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /home/ghansham/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/ghansham/.local/lib/python3.8/site-packages (from nltk) (2022.7.25)\n",
            "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCKaXkjlgtNY",
        "outputId": "f65a8bad-9840-4227-cb5e-dd5a8fc3745d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /home/ghansham/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5ByFOR1g60I",
        "outputId": "40d2b55c-a793-40b9-ec86-e9389bc664e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyD-SoU2hM3a",
        "outputId": "f1d00708-dc1b-49a7-962e-9edcab59fb81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brown.sents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LX59pN0Qhu7_"
      },
      "outputs": [],
      "source": [
        "#Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vEAKssuntDv",
        "outputId": "95ef195e-6049-444f-a95f-ea19d653a435"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ghansham/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFUpZlOcozw_",
        "outputId": "fc2ddf62-0366-4710-eeca-d26b99b3c50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"God is Great! I won a lottery.\"\n",
        "print(word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc2UWY-o--N",
        "outputId": "1a05a575-8145-44fe-e91f-d35ef0c72be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sub-module available for the above is sent_tokenize.', 'An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization.', 'Imagine you need to count average words per sentence, how you will calculate?', 'For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio.', 'Such output serves as an important feature for machine training as the answer would be numeric.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\"\n",
        "print(sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADDYofVapSkx",
        "outputId": "ca12852e-e48f-48c1-93fb-84ee935aed4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(len(sent_tokenize(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9BoR3VFpsTz",
        "outputId": "54216a2e-3880-4516-e177-320970522abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sub-module', 'available', 'for', 'the', 'above', 'is', 'sent_tokenize', '.', 'An', 'obvious', 'question', 'in', 'your', 'mind', 'would', 'be', 'why', 'sentence', 'tokenization', 'is', 'needed', 'when', 'we', 'have', 'the', 'option', 'of', 'word', 'tokenization', '.', 'Imagine', 'you', 'need', 'to', 'count', 'average', 'words', 'per', 'sentence', ',', 'how', 'you', 'will', 'calculate', '?', 'For', 'accomplishing', 'such', 'a', 'task', ',', 'you', 'need', 'both', 'NLTK', 'sentence', 'tokenizer', 'as', 'well', 'as', 'NLTK', 'word', 'tokenizer', 'to', 'calculate', 'the', 'ratio', '.', 'Such', 'output', 'serves', 'as', 'an', 'important', 'feature', 'for', 'machine', 'training', 'as', 'the', 'answer', 'would', 'be', 'numeric', '.']\n",
            "85\n",
            "Sub-module  :  sub-modul\n",
            "available  :  avail\n",
            "for  :  for\n",
            "the  :  the\n",
            "above  :  abov\n",
            "is  :  is\n",
            "sent_tokenize  :  sent_token\n",
            ".  :  .\n",
            "An  :  an\n",
            "obvious  :  obviou\n",
            "question  :  question\n",
            "in  :  in\n",
            "your  :  your\n",
            "mind  :  mind\n",
            "would  :  would\n",
            "be  :  be\n",
            "why  :  whi\n",
            "sentence  :  sentenc\n",
            "tokenization  :  token\n",
            "is  :  is\n",
            "needed  :  need\n",
            "when  :  when\n",
            "we  :  we\n",
            "have  :  have\n",
            "the  :  the\n",
            "option  :  option\n",
            "of  :  of\n",
            "word  :  word\n",
            "tokenization  :  token\n",
            ".  :  .\n",
            "Imagine  :  imagin\n",
            "you  :  you\n",
            "need  :  need\n",
            "to  :  to\n",
            "count  :  count\n",
            "average  :  averag\n",
            "words  :  word\n",
            "per  :  per\n",
            "sentence  :  sentenc\n",
            ",  :  ,\n",
            "how  :  how\n",
            "you  :  you\n",
            "will  :  will\n",
            "calculate  :  calcul\n",
            "?  :  ?\n",
            "For  :  for\n",
            "accomplishing  :  accomplish\n",
            "such  :  such\n",
            "a  :  a\n",
            "task  :  task\n",
            ",  :  ,\n",
            "you  :  you\n",
            "need  :  need\n",
            "both  :  both\n",
            "NLTK  :  nltk\n",
            "sentence  :  sentenc\n",
            "tokenizer  :  token\n",
            "as  :  as\n",
            "well  :  well\n",
            "as  :  as\n",
            "NLTK  :  nltk\n",
            "word  :  word\n",
            "tokenizer  :  token\n",
            "to  :  to\n",
            "calculate  :  calcul\n",
            "the  :  the\n",
            "ratio  :  ratio\n",
            ".  :  .\n",
            "Such  :  such\n",
            "output  :  output\n",
            "serves  :  serv\n",
            "as  :  as\n",
            "an  :  an\n",
            "important  :  import\n",
            "feature  :  featur\n",
            "for  :  for\n",
            "machine  :  machin\n",
            "training  :  train\n",
            "as  :  as\n",
            "the  :  the\n",
            "answer  :  answer\n",
            "would  :  would\n",
            "be  :  be\n",
            "numeric  :  numer\n",
            ".  :  .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "ps = PorterStemmer()\n",
        "paragraph = \"Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\"\n",
        "\n",
        "#tokenizing the paragraph\n",
        "token=word_tokenize(paragraph)\n",
        "print(token)\n",
        "print(len(token))\n",
        "# We will stem above tokenized paragraph\n",
        "for w in token:\n",
        "    print(w, \" : \", ps.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "likFHMC5q5RO",
        "outputId": "93d7d119-b87c-4e97-93c8-2027cc8a579c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sub-module', 'available', 'for', 'the', 'above', 'is', 'sent_tokenize', '.', 'An', 'obvious', 'question', 'in', 'your', 'mind', 'would', 'be', 'why', 'sentence', 'tokenization', 'is', 'needed', 'when', 'we', 'have', 'the', 'option', 'of', 'word', 'tokenization', '.', 'Imagine', 'you', 'need', 'to', 'count', 'average', 'words', 'per', 'sentence', ',', 'how', 'you', 'will', 'calculate', '?', 'For', 'accomplishing', 'such', 'a', 'task', ',', 'you', 'need', 'both', 'NLTK', 'sentence', 'tokenizer', 'as', 'well', 'as', 'NLTK', 'word', 'tokenizer', 'to', 'calculate', 'the', 'ratio', '.', 'Such', 'output', 'serves', 'as', 'an', 'important', 'feature', 'for', 'machine', 'training', 'as', 'the', 'answer', 'would', 'be', 'numeric', '.']\n",
            "85\n",
            "Sub-module  :  sub-module\n",
            "available  :  avail\n",
            "for  :  for\n",
            "the  :  the\n",
            "above  :  abov\n",
            "is  :  is\n",
            "sent_tokenize  :  sent_tokenize\n",
            ".  :  .\n",
            "An  :  an\n",
            "obvious  :  obvy\n",
            "question  :  quest\n",
            "in  :  in\n",
            "your  :  yo\n",
            "mind  :  mind\n",
            "would  :  would\n",
            "be  :  be\n",
            "why  :  why\n",
            "sentence  :  sent\n",
            "tokenization  :  tok\n",
            "is  :  is\n",
            "needed  :  nee\n",
            "when  :  when\n",
            "we  :  we\n",
            "have  :  hav\n",
            "the  :  the\n",
            "option  :  opt\n",
            "of  :  of\n",
            "word  :  word\n",
            "tokenization  :  tok\n",
            ".  :  .\n",
            "Imagine  :  imagin\n",
            "you  :  you\n",
            "need  :  nee\n",
            "to  :  to\n",
            "count  :  count\n",
            "average  :  av\n",
            "words  :  word\n",
            "per  :  per\n",
            "sentence  :  sent\n",
            ",  :  ,\n",
            "how  :  how\n",
            "you  :  you\n",
            "will  :  wil\n",
            "calculate  :  calc\n",
            "?  :  ?\n",
            "For  :  for\n",
            "accomplishing  :  accompl\n",
            "such  :  such\n",
            "a  :  a\n",
            "task  :  task\n",
            ",  :  ,\n",
            "you  :  you\n",
            "need  :  nee\n",
            "both  :  both\n",
            "NLTK  :  nltk\n",
            "sentence  :  sent\n",
            "tokenizer  :  tok\n",
            "as  :  as\n",
            "well  :  wel\n",
            "as  :  as\n",
            "NLTK  :  nltk\n",
            "word  :  word\n",
            "tokenizer  :  tok\n",
            "to  :  to\n",
            "calculate  :  calc\n",
            "the  :  the\n",
            "ratio  :  ratio\n",
            ".  :  .\n",
            "Such  :  such\n",
            "output  :  output\n",
            "serves  :  serv\n",
            "as  :  as\n",
            "an  :  an\n",
            "important  :  import\n",
            "feature  :  feat\n",
            "for  :  for\n",
            "machine  :  machin\n",
            "training  :  train\n",
            "as  :  as\n",
            "the  :  the\n",
            "answer  :  answ\n",
            "would  :  would\n",
            "be  :  be\n",
            "numeric  :  num\n",
            ".  :  .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "ls = LancasterStemmer()\n",
        "paragraph = \"Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\"\n",
        "\n",
        "#tokenizing the paragraph\n",
        "token=word_tokenize(paragraph)\n",
        "print(token)\n",
        "print(len(token))\n",
        "# We will stem above tokenized paragraph\n",
        "for w in token:\n",
        "    print(w, \" : \", ls.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L2lNpUdtnLj",
        "outputId": "ce36eba5-b261-45a2-c2b5-eaff2d26f742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sub-module', 'available', 'for', 'the', 'above', 'is', 'sent_tokenize', '.', 'An', 'obvious', 'question', 'in', 'your', 'mind', 'would', 'be', 'why', 'sentence', 'tokenization', 'is', 'needed', 'when', 'we', 'have', 'the', 'option', 'of', 'word', 'tokenization', '.', 'Imagine', 'you', 'need', 'to', 'count', 'average', 'words', 'per', 'sentence', ',', 'how', 'you', 'will', 'calculate', '?', 'For', 'accomplishing', 'such', 'a', 'task', ',', 'you', 'need', 'both', 'NLTK', 'sentence', 'tokenizer', 'as', 'well', 'as', 'NLTK', 'word', 'tokenizer', 'to', 'calculate', 'the', 'ratio', '.', 'Such', 'output', 'serves', 'as', 'an', 'important', 'feature', 'for', 'machine', 'training', 'as', 'the', 'answer', 'would', 'be', 'numeric', '.']\n",
            "85\n",
            "Sub-module  :  sub-module\n",
            "available  :  avail\n",
            "for  :  for\n",
            "the  :  the\n",
            "above  :  abov\n",
            "is  :  is\n",
            "sent_tokenize  :  sent_tokenize\n",
            ".  :  .\n",
            "An  :  an\n",
            "obvious  :  obvy\n",
            "question  :  quest\n",
            "in  :  in\n",
            "your  :  yo\n",
            "mind  :  mind\n",
            "would  :  would\n",
            "be  :  be\n",
            "why  :  why\n",
            "sentence  :  sent\n",
            "tokenization  :  tok\n",
            "is  :  is\n",
            "needed  :  nee\n",
            "when  :  when\n",
            "we  :  we\n",
            "have  :  hav\n",
            "the  :  the\n",
            "option  :  opt\n",
            "of  :  of\n",
            "word  :  word\n",
            "tokenization  :  tok\n",
            ".  :  .\n",
            "Imagine  :  imagin\n",
            "you  :  you\n",
            "need  :  nee\n",
            "to  :  to\n",
            "count  :  count\n",
            "average  :  av\n",
            "words  :  word\n",
            "per  :  per\n",
            "sentence  :  sent\n",
            ",  :  ,\n",
            "how  :  how\n",
            "you  :  you\n",
            "will  :  wil\n",
            "calculate  :  calc\n",
            "?  :  ?\n",
            "For  :  for\n",
            "accomplishing  :  accompl\n",
            "such  :  such\n",
            "a  :  a\n",
            "task  :  task\n",
            ",  :  ,\n",
            "you  :  you\n",
            "need  :  nee\n",
            "both  :  both\n",
            "NLTK  :  nltk\n",
            "sentence  :  sent\n",
            "tokenizer  :  tok\n",
            "as  :  as\n",
            "well  :  wel\n",
            "as  :  as\n",
            "NLTK  :  nltk\n",
            "word  :  word\n",
            "tokenizer  :  tok\n",
            "to  :  to\n",
            "calculate  :  calc\n",
            "the  :  the\n",
            "ratio  :  ratio\n",
            ".  :  .\n",
            "Such  :  such\n",
            "output  :  output\n",
            "serves  :  serv\n",
            "as  :  as\n",
            "an  :  an\n",
            "important  :  import\n",
            "feature  :  feat\n",
            "for  :  for\n",
            "machine  :  machin\n",
            "training  :  train\n",
            "as  :  as\n",
            "the  :  the\n",
            "answer  :  answ\n",
            "would  :  would\n",
            "be  :  be\n",
            "numeric  :  num\n",
            ".  :  .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer \n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "ss = LancasterStemmer()\n",
        "paragraph = \"Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both NLTK sentence tokenizer as well as NLTK word tokenizer to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\"\n",
        "\n",
        "#tokenizing the paragraph\n",
        "token=word_tokenize(paragraph)\n",
        "print(token)\n",
        "print(len(token))\n",
        "# We will stem above tokenized paragraph\n",
        "for w in token:\n",
        "    print(w, \" : \", ss.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo07pvQluOae",
        "outputId": "104420a1-d244-4a3b-b1eb-1c967bfaff83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/ghansham/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/ghansham/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Lammetizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Zqeytwujz3",
        "outputId": "99e1682b-2dad-4b6b-d746-f3a105431c95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The striped bats are hanging on their feet for best\n",
            "51\n",
            "T  :  T\n",
            "h  :  h\n",
            "e  :  e\n",
            "   :   \n",
            "s  :  s\n",
            "t  :  t\n",
            "r  :  r\n",
            "i  :  i\n",
            "p  :  p\n",
            "e  :  e\n",
            "d  :  d\n",
            "   :   \n",
            "b  :  b\n",
            "a  :  a\n",
            "t  :  t\n",
            "s  :  s\n",
            "   :   \n",
            "a  :  a\n",
            "r  :  r\n",
            "e  :  e\n",
            "   :   \n",
            "h  :  h\n",
            "a  :  a\n",
            "n  :  n\n",
            "g  :  g\n",
            "i  :  i\n",
            "n  :  n\n",
            "g  :  g\n",
            "   :   \n",
            "o  :  o\n",
            "n  :  n\n",
            "   :   \n",
            "t  :  t\n",
            "h  :  h\n",
            "e  :  e\n",
            "i  :  i\n",
            "r  :  r\n",
            "   :   \n",
            "f  :  f\n",
            "e  :  e\n",
            "e  :  e\n",
            "t  :  t\n",
            "   :   \n",
            "f  :  f\n",
            "o  :  o\n",
            "r  :  r\n",
            "   :   \n",
            "b  :  b\n",
            "e  :  e\n",
            "s  :  s\n",
            "t  :  t\n"
          ]
        }
      ],
      "source": [
        "# import these modules\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        " \n",
        "wnl = WordNetLemmatizer()\n",
        "sentence = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "#tokenizing the paragraph\n",
        "token=word_tokenize(paragraph)\n",
        "print(sentence)\n",
        "print(len(sentence))\n",
        "# We will lemmetize above tokenized paragraph\n",
        "for w in sentence:\n",
        "    print(w, \" : \", wnl.lemmatize(w))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4u1r6_5vIKe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "nltk1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
