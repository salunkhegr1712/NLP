{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing / checking that is nltk is there in the system if not this will install it \n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after downloading the nltk we will import it in our system \n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Read function is \n",
      "24906\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# getting data from the file to as a string and we will process it further \n",
    "file1 = open(\"InputDataset.txt\",\"r+\") \n",
    "  \n",
    "print(\"Output of Read function is \")\n",
    "c=file1.read()\n",
    "# print(c)\n",
    "# printing the len of file and the type of after reading through the file \n",
    "print(len(c))\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ghansham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is showing error so we download the punkt to avoid this error \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for your info tokenizer are present inside nltk.tokenize from nltk.tokenize you can get word_tokenize and other tokenizers \n",
    "# on basis of spaces and on basis of spaces tokens are getting created\n",
    "# so as i need the words and the punchuations im using word tokenization\n",
    "from nltk import word_tokenize\n",
    "tokens=word_tokenize(c) # tokenize with word tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5083\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing the tweetTokenizer and then we will process it further \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweetTokens=TweetTokenizer(tokens) # this will give a tokenizer object as the result \n",
    "# now convert the tokens into the list or array \n",
    "tweetTokens=tweetTokens.tokenize(c)\n",
    "pass\n",
    "# print(tweetTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(token):\n",
    "    l=[]\n",
    "    f=token.index(\"'\")\n",
    "    if token==\"'\":\n",
    "        return [\"'\"]\n",
    "    if(token[f+1]==\"s\" or token[f+1]==\"S\"):\n",
    "        l.append(token[:f])\n",
    "        l.append(\"is\")\n",
    "    elif(token[f+1]==\"t\" or token[f+1]==\"T\"):\n",
    "        if(token[:f]==\"can\" or token[:f]==\"Can\" ):\n",
    "            l.append(\"can\")\n",
    "            l.append(\"not\")\n",
    "        elif(token[:f]==\"won\" or token[:f]==\"Wont\" ):\n",
    "            l.append(\"would\")\n",
    "            l.append(\"not\")\n",
    "        else:\n",
    "            l.append(token[:f-1])\n",
    "            l.append(\"not\")\n",
    "    elif(token[f+1]==\"v\" or token[f+1]==\"V\"):\n",
    "        l.append(token[:f-1])\n",
    "        l.append(\"have\")\n",
    "    elif(token[f+1]==\"m\" or token[f+1]==\"M\"):\n",
    "        l.append(token[:f])\n",
    "        l.append(\"am\")\n",
    "    elif(token[f+1]==\"l\" or token[f+1]==\"L\"):\n",
    "        l.append(token[:f])\n",
    "        l.append(\"will\")\n",
    "    elif(token[f+1]==\"d\" or token[f+1]==\"D\"):\n",
    "        l.append(token[:f])\n",
    "        l.append(\"had\")\n",
    "    else:\n",
    "        l.append(token)\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing(\"It's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(parsing(\"i'd\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghansham', 'Salunkhe']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dash(sm):\n",
    "    a=sm.index(\"-\")\n",
    "    return [sm[:a],sm[a+1:]]\n",
    "split_dash(\"Ghansham-Salunkhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghansham', 'Rajaram', 'Salunkhe']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python3 program Split camel case\n",
    "# string to individual strings\n",
    "def camel_case_split(str1):\n",
    "    l=[]\n",
    "    n=0\n",
    "    if (str1.isalpha()):\n",
    "        s=str1[0]\n",
    "        n=n+1\n",
    "    else:\n",
    "        l.append(str1[0])\n",
    "        s=str1[1]\n",
    "        n=2\n",
    "\n",
    "    for i in range(n,len(str1)):\n",
    "        if(str1[i].isupper()):\n",
    "            l.append(s)\n",
    "            s=\"\"\n",
    "        s=s+str1[i]\n",
    "    l.append(s)\n",
    "    return l\n",
    "\n",
    "camel_case_split(\"GhanshamRajaramSalunkhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check for words having camel case or not \n",
    "def checkForCamelCase(s):\n",
    "    return s != s.lower() and s != s.upper() and \"_\" not in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count capital letters inside the character\n",
    "def countUpperCharacters(s,n):\n",
    "    count=0\n",
    "    for i in range(n,len(s)):\n",
    "        if(s[i].isupper()):\n",
    "            count=count+1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Ghansham', 'Rajaram', 'Salunkhe']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code where we bring all things together and then we will return\n",
    "def breakCamelCase(token):\n",
    "    if(token[0]==\"@\" or token[0]==\"#\") and checkForCamelCase(token) and countUpperCharacters(token,2) >0 and \"/\" not in token and \"-\" not in token :\n",
    "        return camel_case_split(token)\n",
    "\n",
    "    elif checkForCamelCase(token) and countUpperCharacters(token,1) >0 and \"/\" not in token and \"-\" not in token:\n",
    "        return camel_case_split(token)\n",
    "    # print(fd)\n",
    "# breakCamelCase(\"#GhanshamRajaramSalunkhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakAndpercent(token):\n",
    "    if token==\"@\":\n",
    "        return ['@']\n",
    "    \n",
    "    return [token[0],token[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses.ascii import isalnum\n",
    "\n",
    "\n",
    "def calculateCharacter(token):\n",
    "    l=0\n",
    "    for i in token:\n",
    "        if isalnum(i):\n",
    "            l+=1\n",
    "    \n",
    "    return len(token)-l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateCharacter(\"ghansham./salunkhe/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuations(token):\n",
    "    \n",
    "    for i in token:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "punctuations(\"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@leighabelle maybe. I doubt it. Lol. Sooooooo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code which gives us the input in the form of oneline at a time \n",
    "with open('Input Dataset.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'who', 'missed', 'Air', 'France', 'flight', 'dies', 'a', ':)', ':)', ':)', 'on', 'date', '2013-09-', '29', 'or', '2013/09', '/', '29']\n"
     ]
    }
   ],
   "source": [
    "# now importing the tweetTokenizer and then we will process it further \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "ttk=TweetTokenizer() # this will give a tokenizer object as the result \n",
    "# now convert the tokens into the list or array \n",
    "ttk=ttk.tokenize(lines[17])\n",
    "print(ttk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "woman\n",
      "who\n",
      "missed\n",
      "Air\n",
      "France\n",
      "flight\n",
      "dies\n",
      "a\n",
      ":)\n",
      ":)\n",
      ":)\n",
      "on\n",
      "date\n",
      "2013-09-\n",
      "2013\n",
      "2013-09-\n",
      "09-\n",
      "29\n",
      "or\n",
      "2013/09\n",
      "/\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(ttk))\n",
    "for i in ttk:\n",
    "\n",
    "    if i[0]==\"@\" or i[0]==\"#\":\n",
    "        for j in camel_case_split(i):\n",
    "            print(j)\n",
    "    \n",
    "    elif \"-\" in i and len(i)>3:\n",
    "        for j in split_dash(i):\n",
    "            print(i)\n",
    "            print(j)\n",
    "        \n",
    "    elif (\"'\" in i and i !=\"'\"):\n",
    "        l=parsing(i)\n",
    "        for j in l:\n",
    "            print(j)\n",
    "            \n",
    "    elif i.count('.')>2 or i.count(',')>2 or i.count('?')>2 or i.count('!')>2:\n",
    "        punctuations(i)\n",
    "\n",
    "    else:\n",
    "        print(i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
