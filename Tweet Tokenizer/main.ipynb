{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing / checking that is nltk is there in the system if not this will install it \n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after downloading the nltk we will import it in our system \n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Read function is \n",
      "24906\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# getting data from the file to as a string and we will process it further \n",
    "file1 = open(\"Input Dataset.txt\",\"r+\") \n",
    "  \n",
    "print(\"Output of Read function is \")\n",
    "c=file1.read()\n",
    "# print(c)\n",
    "# printing the len of file and the type of after reading through the file \n",
    "print(len(c))\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ghansham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is showing error so we download the punkt to avoid this error \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for your info tokenizer are present inside nltk.tokenize from nltk.tokenize you can get word_tokenize and other tokenizers \n",
    "# on basis of spaces and on basis of spaces tokens are getting created\n",
    "# so as i need the words and the punchuations im using word tokenization\n",
    "from nltk import word_tokenize\n",
    "tokens=word_tokenize(c) # tokenize with word tokenize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5083\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now importing the tweetTokenizer and then we will process it further \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweetTokens=TweetTokenizer(tokens) # this will give a tokenizer object as the result \n",
    "# now convert the tokens into the list or array \n",
    "tweetTokens=tweetTokens.tokenize(c)\n",
    "pass\n",
    "# print(tweetTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing(tweetTokens):\n",
    "    fd=[]\n",
    "    for i in tweetTokens:\n",
    "        if \"'\" in i and len(i)>1:\n",
    "            fd.append(i)\n",
    "    print(fd)\n",
    "    l=[]\n",
    "    \n",
    "    for i in range(len(fd)):\n",
    "        f=fd[i].index(\"'\")\n",
    "        if(fd[i][f+1]==\"s\" or fd[i][f+1]==\"S\"):\n",
    "            l.append(fd[i][:f])\n",
    "            l.append(\"is\")\n",
    "        elif(fd[i][f+1]==\"t\" or fd[i][f+1]==\"T\"):\n",
    "            if(fd[i][:f]==\"can\" or fd[i][:f]==\"Can\" ):\n",
    "                l.append(\"can\")\n",
    "                l.append(\"not\")\n",
    "            elif(fd[i][:f]==\"won\" or fd[i][:f]==\"Wont\" ):\n",
    "                l.append(\"would\")\n",
    "                l.append(\"not\")\n",
    "            else:\n",
    "                l.append(fd[i][:f-1])\n",
    "                l.append(\"not\")\n",
    "        elif(fd[i][f+1]==\"v\" or fd[i][f+1]==\"V\"):\n",
    "            l.append(fd[i][:f-1])\n",
    "            l.append(\"have\")\n",
    "        elif(fd[i][f+1]==\"m\" or fd[i][f+1]==\"M\"):\n",
    "            l.append(fd[i][:f])\n",
    "            l.append(\"am\")\n",
    "        elif(fd[i][f+1]==\"l\" or fd[i][f+1]==\"L\"):\n",
    "            l.append(fd[i][:f])\n",
    "            l.append(\"will\")\n",
    "        elif(fd[i][f+1]==\"d\" or fd[i][f+1]==\"D\"):\n",
    "            l.append(fd[i][:f])\n",
    "            l.append(\"had\")\n",
    "        else:\n",
    "            l.append(fd[i])\n",
    "    \n",
    "    for i in l:\n",
    "        print(i)\n",
    "\n",
    "# print(len(gg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ghansham', 'Salunkhe')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dash(sm):\n",
    "    a=sm.index(\"-\")\n",
    "    return sm[:a],sm[a+1:]\n",
    "split_dash(\"Ghansham-Salunkhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code will give you word with having seperated by dash \n",
    "def splitDashedElements(tweetTokens):\n",
    "    gd=[]\n",
    "    for i in tweetTokens:\n",
    "        if \"-\" in i and len(i)>1 and i[0:4]!=\"http\" and i.isalpha()==False:\n",
    "            # gd.append(i)\n",
    "            print(split_dash(i))\n",
    "\n",
    "# gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghansham', 'Rajaram', 'Salunkhe']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python3 program Split camel case\n",
    "# string to individual strings\n",
    "def camel_case_split(str1):\n",
    "    l=[]\n",
    "    n=0\n",
    "    if (str1.isalpha()):\n",
    "        s=str1[0]\n",
    "        n=n+1\n",
    "    else:\n",
    "        l.append(str1[0])\n",
    "        s=str1[1]\n",
    "        n=2\n",
    "\n",
    "    for i in range(n,len(str1)):\n",
    "        if(str1[i].isupper()):\n",
    "            l.append(s)\n",
    "            s=\"\"\n",
    "        s=s+str1[i]\n",
    "    l.append(s)\n",
    "    return l\n",
    "\n",
    "camel_case_split(\"GhanshamRajaramSalunkhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will check for words having camel case or not \n",
    "def checkForCamelCase(s):\n",
    "    return s != s.lower() and s != s.upper() and \"_\" not in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count capital letters inside the character\n",
    "def countUpperCharacters(s,n):\n",
    "    count=0\n",
    "    for i in range(n,len(s)):\n",
    "        if(s[i].isupper()):\n",
    "            count=count+1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakCamelCase(tweetTokens):\n",
    "    cc=[]\n",
    "    for i in tweetTokens:\n",
    "        if(i[0]==\"@\" or i[0]==\"#\") and checkForCamelCase(i) and countUpperCharacters(i,2) >0 and \"/\" not in i and \"-\" not in i:\n",
    "            print(camel_case_split(i))\n",
    "\n",
    "        elif checkForCamelCase(i) and countUpperCharacters(i,1) >0 and \"/\" not in i and \"-\" not in i:\n",
    "            print(camel_case_split(i))\n",
    "breakCamelCase(\"#Ghansham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#GhanshamRajaramSalunkhe']\n",
      "['#', 'Ghansham', 'Rajaram', 'Salunkhe']\n"
     ]
    }
   ],
   "source": [
    "# code which gives us the input in the form of oneline at a time \n",
    "with open('Input Dataset.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "aa=lines[4]\n",
    "\n",
    "ttk=tk.tokenize(\"#GhanshamRajaramSalunkhe\")\n",
    "print(ttk)\n",
    "breakCamelCase(ttk)\n",
    "# splitDashedElements(ttk)\n",
    "# parsing(ttk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
